{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\House Prices Advanced Regression Techniques\\house-prices-advanced-regression-techniques\\train.csv'\n",
    "test_file = r'C:\\Users\\Anjana Tiha\\Drive D\\Programming\\Projects\\House Prices Advanced Regression Techniques\\house-prices-advanced-regression-techniques\\test.csv'\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "# train = pd.read_csv(\"../input/train.csv\")\n",
    "# test = pd.read_csv(\"../input/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = train+test\n",
    "data = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>472.980137</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>213.804841</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>334.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000   \n",
       "\n",
       "          ...        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch  \\\n",
       "count     ...       1460.000000  1460.000000  1460.000000    1460.000000   \n",
       "mean      ...        472.980137    94.244521    46.660274      21.954110   \n",
       "std       ...        213.804841   125.338794    66.256028      61.119149   \n",
       "min       ...          0.000000     0.000000     0.000000       0.000000   \n",
       "25%       ...        334.500000     0.000000     0.000000       0.000000   \n",
       "50%       ...        480.000000     0.000000    25.000000       0.000000   \n",
       "75%       ...        576.000000   168.000000    68.000000       0.000000   \n",
       "max       ...       1418.000000   857.000000   547.000000     552.000000   \n",
       "\n",
       "         3SsnPorch  ScreenPorch     PoolArea       MiscVal       MoSold  \\\n",
       "count  1460.000000  1460.000000  1460.000000   1460.000000  1460.000000   \n",
       "mean      3.409589    15.060959     2.758904     43.489041     6.321918   \n",
       "std      29.317331    55.757415    40.177307    496.123024     2.703626   \n",
       "min       0.000000     0.000000     0.000000      0.000000     1.000000   \n",
       "25%       0.000000     0.000000     0.000000      0.000000     5.000000   \n",
       "50%       0.000000     0.000000     0.000000      0.000000     6.000000   \n",
       "75%       0.000000     0.000000     0.000000      0.000000     8.000000   \n",
       "max     508.000000   480.000000   738.000000  15500.000000    12.000000   \n",
       "\n",
       "            YrSold  \n",
       "count  1460.000000  \n",
       "mean   2007.815753  \n",
       "std       1.328095  \n",
       "min    2006.000000  \n",
       "25%    2007.000000  \n",
       "50%    2008.000000  \n",
       "75%    2009.000000  \n",
       "max    2010.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "target = data[target_col]\n",
    "\n",
    "# test_y= test[target_col]\n",
    "test_y= None\n",
    "\n",
    "data.drop(columns=target_col, inplace=True)\n",
    "\n",
    "features = data\n",
    "\n",
    "rating_cols = ['ExterQual' 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "rating_score = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n",
    "rating_cols2 = {'BsmtFinType1', 'BsmtFinType2'}\n",
    "rating_score2 = {'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0}\n",
    "\n",
    "ignore_cols = ['BsmtExposure']\n",
    "features.drop(columns=ignore_cols, inplace=True)\n",
    "test_x= test.drop(columns=ignore_cols, inplace=True)\n",
    "\n",
    "\n",
    "# print(data.isnull().sum(axis=1))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "normalizer.fit([target])\n",
    "\n",
    "target = normalizer.transform([target]).flatten().astype('float64')\n",
    "\n",
    "features.describe()\n",
    "# target.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
      "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
      "       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
      "       'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',\n",
      "       'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
      "       'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'],\n",
      "      dtype='object')\n",
      "Index(['LotFrontage', 'MasVnrArea', 'GarageYrBlt'], dtype='object')\n",
      "Index(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
      "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
      "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
      "       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
      "       'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n",
      "       'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
      "       'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n",
      "       'SaleCondition'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# print(data.isnull().sum(axis=1))   \n",
    "\n",
    "features.replace(-np.Inf, np.nan)\n",
    "features.replace(np.Inf, np.nan)\n",
    "features.replace(np.nan, 0)\n",
    "features = features.fillna(0)\n",
    "\n",
    "dtype_grp = features.columns.to_series().groupby(data.dtypes).groups\n",
    "\n",
    "for dtype in dtype_grp:\n",
    "    print(dtype_grp[dtype])\n",
    "\n",
    "    \n",
    "for dtype in dtype_grp:\n",
    "    if dtype=='int64' :\n",
    "        for col in dtype_grp[dtype]:\n",
    "            features[col] = features[col].fillna(0.0)\n",
    "            features[col]=preprocessing.normalize([features[col]], norm='l2').flatten()\n",
    "            features[col] = features[col].astype('float64')\n",
    "    elif dtype=='float64' :\n",
    "        for col in dtype_grp[dtype]:\n",
    "            features[col] = features[col].fillna(0.0)\n",
    "            features[col]=preprocessing.normalize([features[col]], norm='l2').flatten()\n",
    "            features[col] = features[col].astype('float64')\n",
    "    elif dtype=='object' :\n",
    "        for col in dtype_grp[dtype]:\n",
    "            if col in rating_cols:\n",
    "                features[col].replace(rating_score, regex=True, inplace=True)\n",
    "                features[col] = features[col].fillna(0.0)\n",
    "                features[col] = features[col].astype('float64')\n",
    "            elif col in rating_cols2:\n",
    "                features[col].replace(rating_score2, regex=True, inplace=True)\n",
    "                features[col] = features[col].fillna(0.0)\n",
    "                features[col] = features[col].astype('float64')\n",
    "            else:\n",
    "                try:\n",
    "                    le = preprocessing.LabelEncoder()\n",
    "                    enc = preprocessing.OneHotEncoder()\n",
    "                    le.fit(features[col].astype(str))\n",
    "                    label_enc_feature_val = le.transform(features[col].astype(str))\n",
    "                    features[col] = label_enc_feature_val.astype('float64')\n",
    "\n",
    "#                     enc.fit([label_enc_feature_val])  \n",
    "#                     hot_enc_feature_val = enc.transform([label_enc_feature_val])\n",
    "\n",
    "#                     print(hot_enc_feature_val)\n",
    "\n",
    "#                     features.join(hot_enc_feature_val)\n",
    "#                     features.drop(columns=col)\n",
    "                except ValueError:\n",
    "                    print(col)\n",
    "                    print(\"Oops!  That was no valid number.  Try again...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.isnull().sum(axis=1))\n",
    "X = features.values\n",
    "Y = target\n",
    "# print(features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "    \n",
    "def model_evaluation(X, Y, X_test, y_test, splitter, model, model_type, report, details):\n",
    "    if model_type==\"classification\":\n",
    "        accuracy = 0\n",
    "        f1 = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "\n",
    "    elif model_type==\"regression\":\n",
    "        explained_variance_score_val = 0\n",
    "        mean_absolute_error_val = 0\n",
    "        mean_squared_error_val = 0\n",
    "        mean_squared_log_error_val = 0\n",
    "        median_absolute_error_val = 0\n",
    "        r2_score_val = 0\n",
    "    \n",
    "    if report:\n",
    "#         print(\"Spliter Description:\")\n",
    "#         print(splitter)\n",
    "#         print(\"-\"*100, \"\\n\")\n",
    "        print(\"Model Description:\")\n",
    "        print(model)\n",
    "        print(\"-\"*100,\"\\n\")\n",
    "      \n",
    "    if splitter:\n",
    "        i=0\n",
    "        for train_index, test_index in splitter.split(X, Y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            predict = model.predict(X_test)\n",
    "            \n",
    "\n",
    "            if model_type==\"classification\":\n",
    "                accuracy_temp = metrics.accuracy_score(y_test, predict)\n",
    "                precision_temp = metrics.precision_score(y_test, predict, average=\"micro\")\n",
    "                recall_temp = metrics.recall_score(y_test, predict, average=\"micro\")\n",
    "                f1_temp = metrics.f1_score(y_test, predict, average=\"micro\")\n",
    "                hamming_loss = metrics.hamming_loss(y_test, predict)\n",
    "\n",
    "                accuracy = accuracy + accuracy_temp\n",
    "                precision = precision + precision_temp\n",
    "                recall = recall+ recall_temp\n",
    "                f1= f1 + f1_temp\n",
    "\n",
    "            elif model_type==\"regression\":\n",
    "                explained_variance_score_temp = metrics.explained_variance_score(y_test, predict)\n",
    "                mean_absolute_error_temp = metrics.mean_absolute_error(y_test, predict)\n",
    "                mean_squared_error_temp = metrics.mean_squared_error(y_test, predict)\n",
    "                mean_squared_log_error_temp = metrics.mean_squared_log_error(y_test, predict)\n",
    "                median_absolute_error_temp = metrics.median_absolute_error(y_test, predict)\n",
    "                r2_score_temp = metrics.r2_score(y_test, predict)\n",
    "\n",
    "\n",
    "                explained_variance_score_val = explained_variance_score_val + explained_variance_score_temp\n",
    "                mean_absolute_error_val = mean_absolute_error_val + mean_absolute_error_temp\n",
    "                mean_squared_error_val = mean_squared_error_val + mean_squared_error_temp\n",
    "                mean_squared_log_error_val = mean_squared_log_error_val + mean_squared_log_error_temp\n",
    "                median_absolute_error_val = median_absolute_error_val + median_absolute_error_temp\n",
    "                r2_score_val = r2_score_val + r2_score_temp\n",
    "            \n",
    "            if details:\n",
    "                print(\"*\"*25,  \" ITERATION - \", i+1, \"*\"*25)\n",
    "#                 print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                if model_type==\"classification\":\n",
    "                    print(\"-\"*35)\n",
    "                    print('%50s%s' % (\"accuracy_score :\", accuracy_temp))\n",
    "                    print('%50s%s' % (\"precision_score :\", precision_score))\n",
    "                    print('%50s%s' % (\"recall_score :\", recall_score))\n",
    "                    print('%50s%s' % (\"f1_score :\", f1_score))\n",
    "                    print('%50s%s' % (\"hamming_loss :\", hamming_loss))\n",
    "                    precision, recall, thresholds = metrics.precision_recall_curve(y_test, predict)\n",
    "                    print('%50s%s' % (\"average_precision_score :\", metrics.average_precision_score(y_test, predict, average=\"micro\")))\n",
    "                    print('%50s%s' % (\"fbeta_score :\", metrics.fbeta_score(y_test, predict)))\n",
    "                    print('%50s%s' % (\"roc_auc_score :\", metrics.roc_auc_score(y_test, predict, average=\"micro\")))\n",
    "                    print(\"-\"*35)\n",
    "                    print(metrics.classification_report(y_test, predict))\n",
    "                    print(\"-\"*35)\n",
    "                    print(\"confusion Matrix:\\n\\n\", metrics.confusion_matrix(y_test, predict))\n",
    "                    print(\"-\"*35)\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                elif model_type==\"regression\":\n",
    "                    print(\"-\"*35)\n",
    "                    print('%50s%s' % (\"explained_variance_score_temp :\", explained_variance_score_temp))\n",
    "                    print('%50s%s' % (\"mean_absolute_error_temp :\", mean_absolute_error_temp))\n",
    "                    print('%50s%s' % (\"mean_squared_error_temp :\", mean_squared_error_temp))\n",
    "                    print('%50s%s' % (\"median_absolute_error_temp :\", median_absolute_error_temp))\n",
    "                    print('%50s%s' % (\"mean_squared_log_error_temp :\", mean_squared_log_error_temp))\n",
    "                    print('%50s%s' % (\"median_absolute_error_temp :\", median_absolute_error_temp))\n",
    "                    print('%50s%s' % (\"r2_score_val :\", r2_score_val))\n",
    "                    print(\"-\"*35)\n",
    "                    print(\"\\n\")\n",
    "            i+=1\n",
    "\n",
    "        split_num = splitter.get_n_splits()\n",
    "        \n",
    "        if model_type==\"classification\":\n",
    "            accuracy = accuracy/split_num\n",
    "            precision = precision/split_num\n",
    "            recall = recall/split_num\n",
    "            f1 = f1/split_num\n",
    "            \n",
    "        elif model_type==\"regression\":\n",
    "            explained_variance_score_val = explained_variance_score_val/split_num\n",
    "            mean_absolute_error_val = mean_absolute_error_val/split_num\n",
    "            mean_squared_error_val = mean_squared_error_val/split_num\n",
    "            median_absolute_error_val = median_absolute_error_val/split_num\n",
    "            r2_score_val = r2_score_val/split_num\n",
    "\n",
    "    else:\n",
    "        model.fit(X, Y)\n",
    "        predict = model.predict(X_test)\n",
    "        \n",
    "        if model_type==\"classification\":\n",
    "            accuracy = metrics.accuracy_score(y_test, predict)\n",
    "            precision = metrics.precision_score(y_test, predict, average=\"micro\")\n",
    "            recall = metrics.recall_score(y_test, predict, average=\"micro\")\n",
    "            f1 = metrics.f1_score(y_test, predict, average=\"micro\")\n",
    "            hamming_loss = metrics.hamming_loss(y_test, predict)\n",
    "            \n",
    "        elif model_type==\"regression\":\n",
    "            explained_variance_score = explained_variance_score(y_test, predict)\n",
    "            mean_absolute_error = mean_absolute_error(y_test, predict)\n",
    "            mean_squared_error = mean_squared_error(y_test, predict)\n",
    "            mean_squared_log_error = mean_squared_log_error(y_test, predict)\n",
    "            median_absolute_error = median_absolute_error(y_test, predict)\n",
    "            r2_score_temp = r2_score(y_test, predict)\n",
    "            \n",
    "    if report:\n",
    "        if splitter:\n",
    "            print(\"*\"*50, \" Average For\", i+1, \" Folds\", \"*\"*50)\n",
    "        if model_type==\"classification\":\n",
    "            print('%50s%s' % (\"Average Accuracy Score: \", accuracy))\n",
    "            print('%50s%s' % (\"Average pPrecision Score: \", precision))\n",
    "            print('%50s%s' % (\"Average Recall Score: \", recall))\n",
    "            print('%50s%s' % (\"Average F1 Score: \", f1))\n",
    "        \n",
    "        elif model_type==\"regression\":\n",
    "            print('%50s%s' % (\"Average explained_variance_score: \", explained_variance_score_val))\n",
    "            print('%50s%s' % (\"Average mean_absolute_error: \", mean_absolute_error_val))\n",
    "            print('%50s%s' % (\"Average mean_squared_error: \", mean_squared_error_val))\n",
    "            print('%50s%s' % (\"Average mean_squared_log_error: \", mean_squared_log_error_val))\n",
    "            print('%50s%s' % (\"Average median_absolute_error: \", median_absolute_error_val))\n",
    "            print('%50s%s' % (\"Average r2_score: \", r2_score_val))\n",
    "            print(\"\\n\")\n",
    "            print(\"*\"*100)\n",
    "            \n",
    "    \n",
    "    if model_type==\"classification\":\n",
    "        return accuracy, precision, recall, f1\n",
    "    elif model_type==\"regression\":\n",
    "        return explained_variance_score_val, mean_absolute_error_val, mean_squared_error_val, mean_squared_log_error_val, median_absolute_error_val, r2_score_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  START  **************************************************\n",
      "Spliter Description:\n",
      "KFold(n_splits=5, random_state=None, shuffle=True)\n",
      "Model Description:\n",
      "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=None)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.8078078618779758\n",
      "                     Average mean_absolute_error: 24193.665874106322\n",
      "                      Average mean_squared_error: 1222722351.3443274\n",
      "                  Average mean_squared_log_error: 0.19867027406195373\n",
      "                   Average median_absolute_error: 17987.42718101145\n",
      "                                Average r2_score: 0.799343090467199\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.8406975328582504\n",
      "                     Average mean_absolute_error: 18778.887260273972\n",
      "                      Average mean_squared_error: 1014278724.4937944\n",
      "                  Average mean_squared_log_error: 0.1166806937937748\n",
      "                   Average median_absolute_error: 11954.429999999998\n",
      "                                Average r2_score: 0.8399883178221247\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best')\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.7379379814511486\n",
      "                     Average mean_absolute_error: 26396.825342465752\n",
      "                      Average mean_squared_error: 1697100884.119863\n",
      "                  Average mean_squared_log_error: 0.21536640474193\n",
      "                   Average median_absolute_error: 17550.0\n",
      "                                Average r2_score: 0.7340879472960848\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.8365728544134571\n",
      "                     Average mean_absolute_error: 18862.253904109588\n",
      "                      Average mean_squared_error: 1028018800.0155684\n",
      "                  Average mean_squared_log_error: 0.12012324094376037\n",
      "                   Average median_absolute_error: 11903.42\n",
      "                                Average r2_score: 0.8362726099183725\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "ExtraTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
      "          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, random_state=None,\n",
      "          splitter='random')\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.6468510826674319\n",
      "                     Average mean_absolute_error: 28920.937671232874\n",
      "                      Average mean_squared_error: 2248534987.3363013\n",
      "                  Average mean_squared_log_error: 0.2532098338849233\n",
      "                   Average median_absolute_error: 18482.0\n",
      "                                Average r2_score: 0.6460634605413712\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "GaussianProcessRegressor(alpha=1e-10, copy_X_train=True, kernel=None,\n",
      "             n_restarts_optimizer=0, normalize_y=False,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=None)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: -0.1601947400538061\n",
      "                     Average mean_absolute_error: 162290.4812473077\n",
      "                      Average mean_squared_error: 33431363613.171925\n",
      "                  Average mean_squared_log_error: 376.6038661332592\n",
      "                   Average median_absolute_error: 146540.22010433045\n",
      "                                Average r2_score: -4.376805153852954\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.8804292266490386\n",
      "                     Average mean_absolute_error: 16079.25534561706\n",
      "                      Average mean_squared_error: 745493453.0476587\n",
      "                  Average mean_squared_log_error: 0.08811832882744197\n",
      "                   Average median_absolute_error: 10398.592652706568\n",
      "                                Average r2_score: 0.8798932198480921\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,\n",
      "        tol=1e-05, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.6753998930475775\n",
      "                     Average mean_absolute_error: 29429.167956609315\n",
      "                      Average mean_squared_error: 2058923736.6042647\n",
      "                  Average mean_squared_log_error: 0.26378707598469553\n",
      "                   Average median_absolute_error: 20383.063436343684\n",
      "                                Average r2_score: 0.6704286214789695\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "KernelRidge(alpha=1, coef0=1, degree=3, gamma=None, kernel='linear',\n",
      "      kernel_params=None)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.721691931943613\n",
      "                     Average mean_absolute_error: 27049.554560064822\n",
      "                      Average mean_squared_error: 1770576327.9843025\n",
      "                  Average mean_squared_log_error: 0.2309191635474814\n",
      "                   Average median_absolute_error: 19259.424465879798\n",
      "                                Average r2_score: 0.7204845022160146\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     random_state=None, tol=0.0001, verbose=0)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.17189697599075998\n",
      "                     Average mean_absolute_error: 51025.44152274246\n",
      "                      Average mean_squared_error: 6073057478.124482\n",
      "                  Average mean_squared_log_error: 0.6767513277260988\n",
      "                   Average median_absolute_error: 33340.079294691735\n",
      "                                Average r2_score: 0.037999815239868465\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana Tiha\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.021608316187862697\n",
      "                     Average mean_absolute_error: 160023.9575406998\n",
      "                      Average mean_squared_error: 31782640909.32975\n",
      "                  Average mean_squared_log_error: 22.371424998573588\n",
      "                   Average median_absolute_error: 142167.16163614503\n",
      "                                Average r2_score: -4.071880429692681\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',\n",
      "   max_iter=-1, nu=0.5, shrinking=True, tol=0.001, verbose=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.0003109282284013526\n",
      "                     Average mean_absolute_error: 55994.43756507421\n",
      "                      Average mean_squared_error: 6388713860.468059\n",
      "                  Average mean_squared_log_error: 0.8019915006396364\n",
      "                   Average median_absolute_error: 41378.60712052095\n",
      "                                Average r2_score: -0.01429876582430012\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "PassiveAggressiveRegressor(C=1.0, average=False, epsilon=0.1,\n",
      "              fit_intercept=True, loss='epsilon_insensitive',\n",
      "              max_iter=None, n_iter=None, random_state=None, shuffle=True,\n",
      "              tol=None, verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.3429300001043355\n",
      "                     Average mean_absolute_error: 42190.010387710296\n",
      "                      Average mean_squared_error: 4399698049.890456\n",
      "                  Average mean_squared_log_error: 0.4591444431697729\n",
      "                   Average median_absolute_error: 27050.51604808861\n",
      "                                Average r2_score: 0.2994619708481562\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana Tiha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.8455190493065283\n",
      "                     Average mean_absolute_error: 18804.90294520548\n",
      "                      Average mean_squared_error: 963952403.5965\n",
      "                  Average mean_squared_log_error: 0.11936374570556824\n",
      "                   Average median_absolute_error: 11832.090000000002\n",
      "                                Average r2_score: 0.8450209520301355\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "**************************************************  Average For 6  Folds **************************************************\n",
      "                Average explained_variance_score: 0.0004119773996853038\n",
      "                     Average mean_absolute_error: 55531.56662580838\n",
      "                      Average mean_squared_error: 6626286831.956838\n",
      "                  Average mean_squared_log_error: 0.7995391128590548\n",
      "                   Average median_absolute_error: 37603.27419161197\n",
      "                                Average r2_score: -0.05083439975301189\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Model Description:\n",
      "TheilSenRegressor(copy_X=True, fit_intercept=True, max_iter=300,\n",
      "         max_subpopulation=10000, n_jobs=1, n_subsamples=None,\n",
      "         random_state=None, tol=0.001, verbose=False)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana Tiha\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py:314: RuntimeWarning: invalid value encountered in log\n",
      "  return mean_squared_error(np.log(y_true + 1), np.log(y_pred + 1),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d5497eeb9d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mregressors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mevaluation_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mexplained_variance_score_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_log_error_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmedian_absolute_error_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0mevaluation_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplained_variance_score_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mevaluation_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_absolute_error_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-94371ebb8bfe>\u001b[0m in \u001b[0;36mmodel_evaluation\u001b[1;34m(X, Y, X_test, y_test, splitter, model, model_type, report, details)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mmean_absolute_error_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mmean_squared_error_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mmean_squared_log_error_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[0mmedian_absolute_error_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mr2_score_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     return mean_squared_error(np.log(y_true + 1), np.log(y_pred + 1),\n\u001b[1;32m--> 315\u001b[1;33m                               sample_weight, multioutput)\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \"\"\"\n\u001b[0;32m    237\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 238\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    239\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[0;32m    240\u001b[0m                                weights=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# classifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomTreesEmbedding, RandomForestClassifier, VotingClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \n",
    "from sklearn.neighbors import KDTree, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# regressor\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomTreesEmbedding, RandomForestRegressor, VotingClassifier)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ARDRegression, LinearRegression, LogisticRegression, LogisticRegressionCV, logistic_regression_path, HuberRegressor, PassiveAggressiveRegressor, RandomizedLogisticRegression, RANSACRegressor, SGDRegressor, TheilSenRegressor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor, NearestNeighbors, RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import BernoulliRBM, MLPRegressor\n",
    "from sklearn.svm import LinearSVR, NuSVR, SVR\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "kf = KFold(n_splits = 5, random_state=None, shuffle =True)\n",
    "\n",
    "classifiers = {\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "#     \"BernoulliRBM\": BernoulliRBM(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n",
    "#     \"GaussianMixture\": GaussianMixture(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "#     \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "#     \"KDTree\": KDTree(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MLPClassifier\": MLPClassifier(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "#     \"NearestNeighbors\": NearestNeighbors(),\n",
    "#     \"NuSVC\": NuSVC(),\n",
    "    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025),\n",
    "    \"SVC\": SVC(),\n",
    "    \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "#     VotingClassifier: VotingClassifier(),\n",
    "}\n",
    "\n",
    "classifiers2 = {\n",
    "#     \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "#     \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(3),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "#     \"NuSVC\": NuSVC(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025)\n",
    "#     \"SVC\": SVC(),\n",
    "#     \"SVC Gamma\": SVC(gamma=2, C=1)\n",
    "}\n",
    "\n",
    "classifiers3 = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025)\n",
    "}\n",
    "\n",
    "\n",
    "regressors = {\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
    "#     \"ARDRegression\": ARDRegression(),\n",
    "    \"BaggingRegressor\": BaggingRegressor(),\n",
    "#     \"BernoulliRBM\": BernoulliRBM(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(),\n",
    "    \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n",
    "#     \"GaussianMixture\": GaussianMixture(),\n",
    "#     \"GaussianNB\": GaussianNB(),\n",
    "    \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "#     \"IsotonicRegression\": IsotonicRegression(),\n",
    "    \"KernelRidge\": KernelRidge(),\n",
    "#     \"KDTree\": KDTree(),\n",
    "#     \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "#     \"LinearRegression\": LinearRegression(), \n",
    "#     \"LogisticRegression\": LogisticRegression(),\n",
    "#     \"LogisticRegressionCV\": LogisticRegressionCV(),\n",
    "#     \"logistic_regression_path\": logistic_regression_path(),\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "    \"MLPRegressor\": MLPRegressor(),\n",
    "#     \"MultinomialNB\": MultinomialNB(),\n",
    "    \"NuSVR\": NuSVR(),\n",
    "    \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n",
    "#     \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "#     \"RadiusNeighborsRegressor\": RadiusNeighborsRegressor(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "#     \"RandomizedLogisticRegression\": RandomizedLogisticRegression(),\n",
    "#     \"RANSACRegressor\": RANSACRegressor(),\n",
    "#     \"SGDRegressor\": SGDRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"TheilSenRegressor\": TheilSenRegressor(),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# splitter = sss\n",
    "splitter = kf\n",
    "# splitter = None\n",
    "model_type = \"regression\"\n",
    "report = 1\n",
    "details = 1\n",
    "evaluation = {}\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "print(\"*\"*50, \" START \", \"*\"*50)\n",
    "print(\"Spliter Description:\")\n",
    "print(splitter)        \n",
    "\n",
    "if model_type==\"classification\":\n",
    "    splitter = sss    \n",
    "    for name in classifiers:\n",
    "        evaluation_temp = []\n",
    "        accuracy, precision, recall, f1 = model_evaluation(X, Y, X_test, y_test, splitter, regressors[name], model_type, report, details=None)\n",
    "        evaluation_temp.append(accuracy)\n",
    "        evaluation_temp.append(precision)\n",
    "        evaluation_temp.append(recall)\n",
    "        evaluation_temp.append(f1)\n",
    "        evaluation[name] = evaluation_temp\n",
    "        gc.collect()\n",
    "        \n",
    "    rows_list = []\n",
    "    \n",
    "    for name in evaluation:\n",
    "        rows_list.append([name]+evaluation[name])\n",
    "    \n",
    "    evaluation_pd = pd.DataFrame(rows_list, columns=['model', 'accuracy', 'precision', 'recall', 'f1']) \n",
    "\n",
    "            \n",
    "elif model_type==\"regression\":\n",
    "    splitter = kf \n",
    "    for name in regressors:\n",
    "        evaluation_temp = []    \n",
    "        explained_variance_score_val, mean_absolute_error_val, mean_squared_error_val, mean_squared_log_error_val, median_absolute_error_val, r2_score_val = model_evaluation(X, Y, X_test, y_test, splitter, regressors[name], model_type, report, details=None)\n",
    "        evaluation_temp.append(explained_variance_score_val)\n",
    "        evaluation_temp.append(mean_absolute_error_val)\n",
    "        evaluation_temp.append(mean_squared_error_val)\n",
    "        evaluation_temp.append(mean_squared_log_error_val)\n",
    "        evaluation_temp.append(median_absolute_error_val)\n",
    "        evaluation_temp.append(r2_score_val)\n",
    "        evaluation[name] = evaluation_temp\n",
    "\n",
    "    rows_list = []\n",
    "\n",
    "    for name in evaluation:\n",
    "        rows_list.append([name]+evaluation[name])\n",
    "\n",
    "    evaluation_pd = pd.DataFrame(rows_list, columns=['Model Name', 'explained_variance_score',  'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error', 'median_absolute_error', 'r2_score']) \n",
    "    \n",
    "    print(\"*\"*50, \" END \", \"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_pd\n",
    "evaluation = evaluation_pd\n",
    "# evaluation.drop(columns='Model Name', inplace=True)\n",
    "evaluation_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "if model_type==\"classification\":\n",
    "    labels= ['accuracy', 'precision', 'recall', 'f1']\n",
    "elif model_type==\"regression\":\n",
    "    labels= ['explained_variance_score',  'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error', 'median_absolute_error', 'r2_score']\n",
    "\n",
    "for n in range(0,6):\n",
    "    figure(num=None, figsize=(14, 6), dpi=250)\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    plt.plot(evaluation['Model Name'],evaluation[labels[n]], label = labels[n])\n",
    "\n",
    "    leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=True, fancybox=True)\n",
    "    plt.xticks(rotation=60)\n",
    "    # leg.get_frame().set_alpha(0.5)\n",
    "    plt.legend()\n",
    "    ax.tick_params(labelsize='large', width=5)\n",
    "    ax.grid(True, linestyle='-.')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('x label')\n",
    "    plt.ylabel('y label')\n",
    "\n",
    "    plt.title(labels[n])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
